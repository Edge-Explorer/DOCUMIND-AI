# DOCUMIND-AI

## Intelligent Offline Document Q&A Assistant

DOCUMIND-AI is an AI-powered **offline document question-answering system** that allows users to upload documents and ask natural language questions about their content. The system uses **Retrieval-Augmented Generation (RAG)** with local LLMs to ensure **privacy, low latency, and zero dependency on cloud APIs**.

This project focuses on **practical AI engineering**: document ingestion, semantic search, OCR, vector indexing, and end-to-end system integration.

---

## ğŸš€ Key Features

* ğŸ“„ Upload and process **PDF, DOCX, and TXT** documents
* ğŸ” Semantic search using **FAISS vector database**
* ğŸ§  Context-aware Q&A using **local LLMs (Ollama)**
* ğŸ–¼ï¸ OCR support for **scanned PDFs** using Tesseract
* ğŸ”’ Fully **offline & privacy-first** architecture
* ğŸ“± Mobile-friendly frontend built with **React Native (Expo)**

---

## ğŸ§  System Architecture

1. **Document Ingestion**
   Uploaded documents are parsed and split into chunks.

2. **OCR Processing**
   Scanned PDFs are processed using Tesseract OCR to extract text.

3. **Embedding & Indexing**
   Text chunks are converted into embeddings and stored in a **FAISS index**.

4. **Query Processing**
   User queries are embedded and matched against the FAISS index.

5. **LLM Response Generation**
   Relevant document context is passed to a local LLM via **LangChain + Ollama**.

---

## ğŸ§© Tech Stack

### Backend

* Python
* Flask
* LangChain
* FAISS
* Ollama (local LLM runtime)
* Tesseract OCR

### Frontend

* React Native
* Expo

---

## ğŸ“‚ Project Structure

```
DOCUMIND-AI/
â”‚
â”œâ”€â”€ app.py                 # Flask backend entry point
â”œâ”€â”€ ollama_llm.py          # Local LLM wrapper (Ollama + LangChain)
â”œâ”€â”€ setup_models.py        # Script to setup required local models
â”œâ”€â”€ utils.py               # Helper functions (OCR, embeddings, file handling)
â”‚
â”œâ”€â”€ document/              # Uploaded documents
â”œâ”€â”€ index_store/           # FAISS vector indexes
â”‚
â”œâ”€â”€ frontend/              # React Native mobile application
â”‚
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # Project documentation
â””â”€â”€ LICENSE                # MIT License
```

---

## ğŸ“Š Design Decisions

* **FAISS** was chosen for fast, in-memory vector similarity search.
* **Local LLMs (Ollama)** ensure data privacy and offline usability.
* **LangChain** simplifies RAG pipeline orchestration.
* **OCR integration** enables handling of real-world scanned documents.

---

## ğŸ“ˆ Performance Notes

* Average query latency depends on model size and hardware
* Optimized for **single-user, local inference**
* Suitable for personal research, study, and document analysis

---

## ğŸ¯ Use Cases

* Academic research paper analysis
* Resume and document review
* Legal or policy document exploration
* Personal knowledge base creation

---

## ğŸ”® Future Improvements

* Add unit and integration tests
* Improve chunking and retrieval accuracy
* Support multi-document conversation memory
* Add admin dashboard for document management
* Deploy backend as a containerized service

---

## ğŸ§‘â€ğŸ’» Author

**Karan Shelar**
GitHub: [https://github.com/Edge-Explorer](https://github.com/Edge-Explorer)

---

## ğŸ“œ License

This project is licensed under the MIT License.



